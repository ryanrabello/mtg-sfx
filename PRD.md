# Magic: The Gathering Commander SoundFX Companion – Product Requirements Document

## Objective

The goal is to create a mobile web application that enhances a multiplayer **Magic: The Gathering – Commander** game by adding immersive sound effects in real time. The app will **passively listen** to the table-talk and gameplay announcements via the phone’s microphone, recognize key game events or terms (e.g. card names like *“Dismember”* or actions like *“destroy that creature”*), and play short, appropriate sound effects to match those events. This adds a layer of fun and atmosphere to the game without players needing to manually trigger sounds. The solution should be easy to set up (just open on a phone and start listening) and not interfere with gameplay, essentially acting as a “smart soundboard” that reacts to the ongoing Commander match.

## Features

* **Real-Time Voice Recognition:** Continuously capture and transcribe spoken words during gameplay using a reliable speech-to-text engine. The app should use the easiest and most effective speech recognition API available (whether a built-in browser API or a cloud service) to convert microphone input into text. For example, modern browsers can leverage the Web Speech API, which uses the device’s default speech recognition service (Siri, Google, etc.) to handle the audio input (note that some browsers send audio to a server for processing, meaning pure offline use may not be possible). The recognition should function for multiple speakers around a table and in potentially noisy environments (typical of casual gameplay). It should operate in a continuous or streaming mode so it can catch phrases without needing constant reactivation. Basic voice activity detection or a hotword might be used to manage the stream if necessary (to avoid overwhelming the system with irrelevant chatter).

* **AI-Driven Sound Decision Making:** Implement an intelligent layer that analyzes the transcribed text and decides **if a sound effect should be played, and which one**. This layer can combine rule-based logic with AI (LLM) reasoning:

  * *Rule-based parsing:* Identify specific **Magic: The Gathering terms** or phrases (e.g., *“destroy”*, *“dies”*, *“counterspell”*, *“dragon”*, *“bird”*, card names like *“Lightning Bolt”* or *“Dismember”*) and map them to predefined sound categories. For example, if the transcript contains *“Destroy target creature”* or the card name *“Dismember”*, the app might classify this as a *“creature death”* event and queue a sound of a creature dying or an explosion. If it hears *“I cast Lightning Bolt”*, it could trigger a *lightning or fire* sound effect. These rules will cover common keywords for dramatic events: creature deaths, spell casts, combat actions, etc.
  * *LLM-based inference:* For more complex or less explicit contexts, utilize a language model via frameworks like **LangChain/LangGraph** to interpret the intent. LangGraph (built on LangChain) is an AI agent framework for orchestrating language model workflows. In this app, an LLM agent can take the recent transcript (e.g., *“Player A: I tap three lands and cast **Wrath of God** to destroy all creatures”*) and infer the appropriate sound category (in this case, a mass destruction sound). The AI model can understand synonyms or creative phrasing that the rule-based system might miss, ensuring sounds trigger even when players use natural language or nicknames for cards. This decision module should be tuned to avoid false positives (for instance, casual conversation unrelated to the game shouldn’t trigger sounds).
  * The combination of rules and AI gives a fallback: the app checks known triggers first for speed, and if none match, it can query the AI agent for a suggestion. The AI could be powered by a local lightweight model or a cloud API (depending on feasibility and performance) integrated through LangChain. We will define a set of **sound categories** (e.g., “Creature Death”, “Spell - Fire”, “Spell - Counter”, “Monster Roar”, “Ambient - Birds”) and the AI’s job is to map the spoken context to one of these categories or to “no sound” if nothing relevant is detected.

* **Sound Effects Playback (via Freesound):** Upon deciding to play a sound, the app will fetch and play a short **sound effect** appropriate for the event:

  * We will integrate with **Freesound.org** (a large open repository of sound clips) via their API to source sound files. Freesound’s API allows querying by keywords to find relevant sounds (e.g., a query for “dragon roar” or “sword clang”). The app will have a predefined set of search queries or specific sound IDs for each sound category to ensure consistency. For example, for a “bird” event, we might use a particular bird chirping clip; for “destroy spell”, a specific explosion sound clip.
  * To reduce latency, the app should **cache sounds** once they are fetched. When a sound is first identified to play, the clip will be downloaded (using Freesound’s *sound preview* links, which provide an MP3/OGG without requiring OAuth authentication). The clip can be stored in memory or local storage so that subsequent uses (e.g., multiple creatures die in a game) play instantly without network delay. We might even pre-fetch certain common sounds at startup (like a set of generic spell sounds and creature death sounds) if the user’s connection allows.
  * Sound playback should be handled seamlessly: use a quick-playing audio method that allows overlapping sounds or queueing if two events trigger in rapid succession. Volume levels should be balanced so the sound effects are noticeable but not disruptive – possibly with a default volume setting and an option for the user to adjust a master volume. The clips will generally be short (1-3 seconds) and should stop or fade out before the next sound plays to avoid cacophony.
  * The app will operate in a **passive listening mode** – it does not produce any speech or interact verbally; it only outputs sound effects. There will be an option to mute or pause sound output quickly (for instance, if a conversation unrelated to the game starts triggering sounds or during a pause in the game).

* **On-Screen Log of Events and Sounds:** The UI will include a visible log that updates whenever a sound is played. Each log entry will show a timestamp and a brief description of what was detected and which sound was triggered. For example, an entry might read: *“\[10:32:45] **Spell cast** – Triggered *Explosion SFX* (Creature destroyed)”*. This provides transparency, so users know why a sound played (helpful if a sound seems out of place, they can check what the app thought it heard). The log can also assist in debugging misheard words or adjusting the sensitivity. The most recent events should appear at the top of the log and it should be scrollable to review earlier events. This log doesn’t need to be saved between sessions (though we could consider exporting it if desired), it’s mainly for real-time reference during the game.

* **Mobile-Friendly Web Interface:** The app is designed to run in a mobile web browser (e.g. Safari/Chrome on a smartphone) as a web page or Progressive Web App (PWA). It should have a **responsive UI** that is functional on a small screen without requiring constant attention. The interface will likely stay on automatically (prevent auto-lock if possible) so the phone can sit on the table running for the duration of the game. All controls and displays are touch-friendly and minimal to avoid distracting the players from the game itself.

## Tech Stack

* **Frontend:** The application will be built with **React** (a JavaScript library for UI) using **Vite** as the build tool. React provides a modular way to build the interface (for the log, controls, etc.), and Vite ensures fast development and optimized production bundling. The choice of React/Vite aligns with modern web development practices and allows for potentially publishing the app as a PWA for offline capable installation.
* **Speech Recognition API:** We will utilize the **Web Speech API** (if available on the user’s device/browser) for realtime speech-to-text. This API is supported in Chrome and some modern browsers and leverages the device’s or browser’s speech recognition engines. It is straightforward to use in JavaScript and does not require separate installations, making setup easy for users. If the Web Speech API is not available or not sufficient (for example, iOS Safari support is partial), we will consider a cloud STT service such as **Google Cloud Speech-to-Text**, **Deepgram**, or **Mozilla DeepSpeech** (self-hosted) as a fallback. The goal is to choose a solution with minimal configuration (no complex user API keys) and that can handle continuous, multi-speaker input. The app will likely start listening when the user taps a “Start” button and continue until stopped, using the API’s continuous mode or restarting it in loops due to browser limitations (since some implementations stop after a few seconds).
* **AI Processing:** For interpreting the transcribed text and mapping it to sounds, the app will incorporate a small AI/logic module. We plan to use **LangChain** or **LangGraph** for this purpose. LangChain provides a framework to integrate large language models (LLMs) and tools in an application, and LangGraph is an extension for building resilient AI agent workflows. In practice, this means we might have a Python service running a language model (like a GPT-3.5/GPT-4 via API, or a local model) which the front-end can send text to, and it returns a decision (e.g., “play dragon\_roar.mp3” or “no\_sound”). However, to keep things simple and fast, the initial implementation might primarily use on-device rule-based logic (simple keyword matching) and only call an LLM via a cloud API for complex cases. We will explore using a lightweight model or a service that can run the decision logic. Because the game terminology is fairly niche (MTG card names and actions), we may supplement the AI with a curated list of Magic terms and their associated sound categories. This acts as a knowledge base the AI can reference (possibly implemented as a prompt or as part of LangChain’s tools).
* **Audio Playback:** The app will make use of Web Audio capabilities for playing sounds. We can use a library like **Howler.js** (a popular audio library) or the React hook **use-sound** (which is a small wrapper around Howler) for convenient audio playback management. For instance, the `use-sound` hook provides a simple way to play sound effects in React with a very small bundle footprint. This will allow us to easily control volume, handle overlapping sounds, and potentially add effects like fade-outs. The sounds will be short MP3/OGG files fetched via URL.
* **Backend/Cloud Components:** Ideally the app runs entirely client-side (for responsiveness and simplicity). The speech recognition and audio playback can happen in the browser. However, if we integrate a sophisticated AI (LLM) for decision-making or need to protect API keys, we might introduce a minimal backend:

  * A cloud function or lightweight server (Node.js or Python Flask) could receive the transcribed text and return a sound suggestion. This server could host the LangChain/LangGraph agent with access to an LLM or a custom rules engine. It can also proxy requests to the Freesound API if we want to keep the API key secret.
  * If using the Freesound API directly from the client, we will include the API token in the requests (the token can be safely embedded for public use if it’s a free key, or we use a proxy if needed). Freesound’s search and preview retrieval can be done via simple HTTPS calls from the browser. We must abide by Freesound’s usage terms (e.g., not exceeding rate limits and providing attribution if required by certain sound licenses).
* **Data Storage & Caching:** The app will use the browser’s storage (like IndexedDB or the Cache API) to store fetched sound files and possibly a history of recent transcripts. Caching sound files locally means we don’t have to re-download the same effect repeatedly. We might also cache a limited transcript or state for the AI module (for example, last command or context window for the LLM agent) if needed for better understanding. If we implement as a PWA, we can cache core assets and even some sounds for offline or poor network scenarios (though voice recognition might not function offline unless the browser supports it).

## APIs or Libraries

* **Web Speech API (SpeechRecognition)** – Used for converting live speech to text in the browser. This is part of the Web API in modern browsers. If unavailable or insufficient:

  * **Alternative STT Services:** *Google Cloud Speech-to-Text API*, *Microsoft Azure Cognitive Services (Speech)*, or *Deepgram API* could be used by streaming the audio. These require network connectivity and API keys, and would be integrated via fetch calls or WebSockets from the app.
  * **Speech Recognition Libraries:** *react-speech-recognition* (a wrapper for Web Speech API) can simplify usage in React, and handle some cross-browser quirks. We will evaluate using this for rapid development.
* **LangChain / LangGraph:** A Python (or JavaScript) library framework for building AI logic with LLMs. We plan to use this to create an agent that parses game context from text. For example, using LangChain, we can construct a prompt that instructs an LLM like: *“You are a Magic: The Gathering sound effect assistant. When given a phrase from a game, you classify it into a sound effect category or 'none'.”* The library will help manage the prompt, model API calls, and possible tool-use (like it could even do a web lookup if needed, though probably not necessary). If we use LangGraph, it might allow a more rule-based graph of decisions (like first check a dictionary, then maybe call an LLM, etc.).
* **Freesound API:** Service to search and retrieve sound effects by keywords. We will use the **Text Search** endpoint of Freesound (e.g., `search/text/?query=magic+explosion&token=APIKEY`) to find relevant sounds, likely done ahead of time for known categories. The API returns JSON including sound metadata and preview URLs. We will specifically use the **Sound Preview** URLs (which are direct links to OGG/MP3 files of the sounds) for quick playback, as these do not require OAuth2 authentication (only the API token in the search request). We will need a Freesound API key; this is free to obtain for non-commercial use. The app might include a default key, or prompt the user to input their own if necessary (but ideally we handle it so the user doesn’t need to do extra setup).
* **React & Related Libraries:** We will use React for the UI. Additional libraries and hooks:

  * **use-sound (NPM)** – A React hook by Josh W. Comeau that simplifies playing sound effects in React apps. It internally uses Howler.js for audio. This will handle loading audio files, playing, pausing, and volume control with minimal code.
  * **date-fns or dayjs** – A lightweight library to format timestamps for the event log.
  * **React Speech Recognition** – (If needed) a wrapper for Web Speech API that provides hooks like `useSpeechRecognition` to easily get the transcript and manage the mic. This can manage continuous listening better in React’s stateful way.
* **State Management:** The app’s state is relatively simple (mainly the listening on/off state, the log entries, and maybe the loaded sounds). We can use React’s built-in state and context. If it grows in complexity, we might introduce a state management library (Redux or Zustand), but likely unnecessary for v1.
* **UI Components/Framework:** We may use a minimal component library or custom components given the simple UI. Possibly something like **Chakra UI** or **Material UI** for easy styling if needed, but we can also do custom CSS since the interface is not very complex.

## UX Mockup Notes

*The interface is simple and focused on the core functionality. Below is a textual mockup of the UI elements and layout on a mobile screen:*

* **Header:** At the top, a title or banner such as “🪄 MTG SoundFX Companion” is displayed to remind the user which app this is. Next to the title, there may be a small indicator icon for the microphone status – for example, a microphone symbol that is **red when actively listening** and grey or crossed-out when paused. Tapping this icon (or an explicit Start/Stop button) toggles the listening state. When listening is active, the app might also show a subtle pulsating animation or equalizer bars to indicate it’s capturing audio.

* **Main Sound Log Area:** The majority of the screen is a scrollable list (newest entries at the top) showing the **history of detected events and played sounds**. Each entry in the log has:

  * A timestamp (hh\:mm\:ss) of when the event was recognized.
  * A short description of the event or the phrase heard, and the sound that was played. The description might be generated by the app. For example:

    * *“\[08:15:30] **Fireball** cast – Played 🔥 Fire Explosion sound.”*
    * *“\[08:17:05] **Dragon** entered – Played 🐉 Dragon Roar sound.”*
    * *“\[08:20:10] No relevant trigger (speech recognized but no sound).”* – In case the app intentionally didn’t play anything (these might be optional to display or could be omitted to reduce noise).
  * Possibly an icon representing the category of sound (like a small flame icon for fire, a skull for death, etc.) to make scanning the log easy.
  * Entries might be color-coded or have an identifying tag (e.g., “Spell”, “Creature”, “Misc”) for the type of event.
  * If the app misheard something and triggered incorrectly, the players can glance at the log to see what it thought it heard, aiding in adjusting how they speak commands or letting the developers refine the logic.

* **Control Bar:** At the bottom of the screen, there will be a small control/status bar. This includes:

  * The **Listen Toggle**: A prominent button to start or stop the voice recognition. For instance, a round button with a mic icon labeled “Listen” (when off) or “Pause” (when on). This gives players an easy way to pause the app if needed (for example, if they take a break or a side conversation starts).
  * A **Volume Slider or Button**: A quick control to adjust or mute the sound effects. Possibly an icon (speaker symbol) that can be tapped to mute/unmute, and if tapped and held (or a slider pops up) to adjust volume. This ensures the sound level can be changed without digging into any settings.
  * (Optional) **Settings Icon**: A small gear icon that opens a settings panel. In settings, a user could do things like: enter their Freesound API key if required, choose which categories of sounds are enabled/disabled (maybe someone doesn’t want ambient sounds, only spell effects), or switch between rule-based mode vs. AI mode, etc. For the MVP, settings can be minimal, but having the icon placeholder will allow future expansion. The settings menu would likely be a simple overlay or page with toggles and inputs, given it’s a mobile UI.

* **Visual Design:** The style should be clean and not overly flashy (so as not to distract from the card game). A dark theme background (charcoal or dark blue) with light text could be used, since many Commander games are played in the evening and a bright white screen could be glaring on the table. The log entries could use a readable mono-spaced font for the timestamp and a clear sans-serif for the description. Important words (like the name of a card or action) might be bolded in the log entry. The use of small thematic icons (magic wand, explosion, creature silhouette) will add flavor but should be used sparingly for clarity.

* **User Interaction:** Apart from tapping the listen toggle or adjusting volume, the user isn’t expected to interact with the app frequently during gameplay. The app mostly runs on its own. If a particular sound is disruptive or unwanted, players can hit pause or mute quickly. After the game, they might scroll through the log for curiosity or to laugh about certain moments that were accentuated by sound. The log could auto-prune after it reaches, say, 50 entries to prevent memory bloat (oldest entries drop off).

* **Mock Flow:**

  1. User navigates to the web app on their phone. They see the title and a “Listen” button. Perhaps a subtitle like “Place near players and tap ‘Listen’ to start” for first-time use.
  2. They tap **Listen**. The microphone icon turns red and maybe a text appears “Listening…”. The game begins and players talk normally.
  3. A player says: *“I cast Wrath of God.”* A couple seconds after, the phone plays an ominous **whooshing boom** sound. The log now shows *“\[time] **Wrath of God** – Played 💥 Mass Destruction sound.”*
  4. The game continues, each time an event triggers a sound, the phone plays it and logs it. If there’s a stretch of conversation with no triggers, nothing plays (the phone might still capture speech but the logic skips it; these moments might not appear in the log unless we decide to log “no trigger” events for transparency).
  5. If at any point the app hears something ambiguous, it might consult the AI – this is invisible to the user, except maybe a 0.5s extra delay before sound. The result either triggers a sound or not.
  6. If a player wants to pause the sounds (maybe during a sensitive strategy discussion), they tap the pause button. The mic icon goes grey, and the app stops listening until resumed.
  7. End of game: the user can scroll the log to recall the fun moments with sound. They can then close the app or leave it for the next game.